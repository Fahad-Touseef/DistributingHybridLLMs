{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "375116b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/r/i/riyad/miniconda3/lib/python3.12/site-packages/mamba_ssm/ops/selective_scan_interface.py:163: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\n",
      "/u/r/i/riyad/miniconda3/lib/python3.12/site-packages/mamba_ssm/ops/selective_scan_interface.py:239: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\n",
      "/u/r/i/riyad/miniconda3/lib/python3.12/site-packages/mamba_ssm/ops/triton/layer_norm.py:985: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\n",
      "/u/r/i/riyad/miniconda3/lib/python3.12/site-packages/mamba_ssm/ops/triton/layer_norm.py:1044: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\n",
      "/u/r/i/riyad/miniconda3/lib/python3.12/site-packages/mamba_ssm/distributed/tensor_parallel.py:25: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\n",
      "/u/r/i/riyad/miniconda3/lib/python3.12/site-packages/mamba_ssm/distributed/tensor_parallel.py:61: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\n",
      "/u/r/i/riyad/miniconda3/lib/python3.12/site-packages/mamba_ssm/ops/triton/ssd_combined.py:757: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\n",
      "/u/r/i/riyad/miniconda3/lib/python3.12/site-packages/mamba_ssm/ops/triton/ssd_combined.py:835: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, JambaForSequenceClassification, JambaConfig\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32910fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "808ab8385875406aa49e3103a89cc2c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "606f0fac547f4fc1977e965ca8f16019",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fca278193e5f4ae484243e9477ee0c94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the IMDB dataset from Hugging Face\n",
    "imdb = load_dataset(\"imdb\")\n",
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ai21labs/Jamba-v0.1\")\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)  # padding left to collator\n",
    "tokenized_imdb = imdb.map(preprocess_function, batched=True)\n",
    "\n",
    "tokenized_imdb = tokenized_imdb.remove_columns([\"text\"])\n",
    "tokenized_imdb.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "#outputs = model.generate(input_ids, max_new_tokens=216)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6bc2143",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    tokenized_imdb[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=8,\n",
    "    collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d83002f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
    "label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0770ec6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,     0,     0,  ...,  2527,  9621, 62959],\n",
       "        [    1,  6245, 62966,  ...,  2793,  8439, 13645],\n",
       "        [    0,     0,     0,  ..., 62936,  2196, 62959],\n",
       "        ...,\n",
       "        [    0,     0,     0,  ...,  1876,  2799, 62959],\n",
       "        [    0,     0,     0,  ...,  1865,  1870, 62959],\n",
       "        [    0,     0,     0,  ...,  1836,  1895, 63052]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [0, 0, 0,  ..., 1, 1, 1],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 1, 1, 1],\n",
       "        [0, 0, 0,  ..., 1, 1, 1],\n",
       "        [0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([1, 1, 0, 1, 0, 1, 1, 0])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3508390f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "JambaForSequenceClassification.__init__() got an unexpected keyword argument 'num_labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 43\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# config = JambaConfig(\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m#     vocab_size=50257,\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m#     dim=512,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m#     expert_layer_period =4, # every 4 layer there is a expert layer\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[1;32m     42\u001b[0m config \u001b[38;5;241m=\u001b[39m JambaConfig(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mjamba_config)\n\u001b[0;32m---> 43\u001b[0m model  \u001b[38;5;241m=\u001b[39m \u001b[43mJambaForSequenceClassification\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: JambaForSequenceClassification.__init__() got an unexpected keyword argument 'num_labels'"
     ]
    }
   ],
   "source": [
    "jamba_config = {\n",
    "    \"vocab_size\": len(tokenizer.vocab),\n",
    "    \"hidden_size\": 256,\n",
    "    \"intermediate_size\": 14336,\n",
    "    \"num_hidden_layers\": 2,\n",
    "    \"num_attention_heads\": 4,\n",
    "    \"num_key_value_heads\": 4,\n",
    "\n",
    "    # Expert / MoE (optional)\n",
    "    \"num_experts_per_tok\": 1,\n",
    "    \"num_experts\": 2,\n",
    "    \"expert_layer_offset\": 1,\n",
    "\n",
    "    # Attention layer config\n",
    "    \"attn_layer_period\": 1,\n",
    "    \"attn_layer_offset\": 1,\n",
    "\n",
    "    # Mamba-specific config\n",
    "    \"use_mamba_kernels\": False,\n",
    "    \"mamba_d_state\": 16,\n",
    "    \"mamba_d_conv\": 4,\n",
    "    \"mamba_expand\": 2,\n",
    "    \"output_router_logits\":False,\n",
    "    \"num_labels\":2,\n",
    "}\n",
    "# config = JambaConfig(\n",
    "#     vocab_size=50257,\n",
    "#     dim=512,\n",
    "#     num_hidden_layers=8, # number of hidden layers in transformer block\n",
    "\n",
    "#     hidden_size=256,\n",
    "#     attn_layer_period = 8, # every 4 layer there is a attention layer\n",
    "#     attention_offset = 0, # offset for attention layer\n",
    "#     num_attention_heads=8,\n",
    "#     num_key_value_heads = 8, # if equal with num_attention_heads, then use MultiheadAttention\n",
    "\n",
    "#     d_conv=4,\n",
    "#     d_state=256,\n",
    "#     num_experts_per_tok = 2,  # a router choosing 2 experts per token\n",
    "#     num_experts=2, # total number of experts\n",
    "#     expert_layer_period =4, # every 4 layer there is a expert layer\n",
    "# )\n",
    "config = JambaConfig(**jamba_config)\n",
    "model  = JambaForSequenceClassification(config, num_labels=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16c7fd58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JambaForSequenceClassification(\n",
       "  (model): JambaModel(\n",
       "    (embed_tokens): Embedding(65536, 256, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0): JambaMambaDecoderLayer(\n",
       "        (mamba): JambaMambaMixer(\n",
       "          (conv1d): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)\n",
       "          (act): SiLU()\n",
       "          (in_proj): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (x_proj): Linear(in_features=512, out_features=48, bias=False)\n",
       "          (dt_proj): Linear(in_features=16, out_features=512, bias=True)\n",
       "          (out_proj): Linear(in_features=512, out_features=256, bias=False)\n",
       "          (dt_layernorm): JambaRMSNorm((16,), eps=1e-06)\n",
       "          (b_layernorm): JambaRMSNorm((16,), eps=1e-06)\n",
       "          (c_layernorm): JambaRMSNorm((16,), eps=1e-06)\n",
       "        )\n",
       "        (feed_forward): JambaMLP(\n",
       "          (gate_proj): Linear(in_features=256, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=256, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=256, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): JambaRMSNorm((256,), eps=1e-06)\n",
       "        (pre_ff_layernorm): JambaRMSNorm((256,), eps=1e-06)\n",
       "      )\n",
       "      (1): JambaMambaDecoderLayer(\n",
       "        (mamba): JambaMambaMixer(\n",
       "          (conv1d): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)\n",
       "          (act): SiLU()\n",
       "          (in_proj): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (x_proj): Linear(in_features=512, out_features=48, bias=False)\n",
       "          (dt_proj): Linear(in_features=16, out_features=512, bias=True)\n",
       "          (out_proj): Linear(in_features=512, out_features=256, bias=False)\n",
       "          (dt_layernorm): JambaRMSNorm((16,), eps=1e-06)\n",
       "          (b_layernorm): JambaRMSNorm((16,), eps=1e-06)\n",
       "          (c_layernorm): JambaRMSNorm((16,), eps=1e-06)\n",
       "        )\n",
       "        (feed_forward): JambaSparseMoeBlock(\n",
       "          (router): Linear(in_features=256, out_features=2, bias=False)\n",
       "          (experts): ModuleList(\n",
       "            (0-1): 2 x JambaMLP(\n",
       "              (gate_proj): Linear(in_features=256, out_features=14336, bias=False)\n",
       "              (up_proj): Linear(in_features=256, out_features=14336, bias=False)\n",
       "              (down_proj): Linear(in_features=14336, out_features=256, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): JambaRMSNorm((256,), eps=1e-06)\n",
       "        (pre_ff_layernorm): JambaRMSNorm((256,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (final_layernorm): JambaRMSNorm((256,), eps=1e-06)\n",
       "  )\n",
       "  (score): Linear(in_features=256, out_features=2, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cc60d2",
   "metadata": {},
   "source": [
    "### Standard Jamba Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77768665",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JambaModel(\n",
       "  (embed_tokens): Embedding(65536, 4096, padding_idx=0)\n",
       "  (layers): ModuleList(\n",
       "    (0): JambaMambaDecoderLayer(\n",
       "      (mamba): JambaMambaMixer(\n",
       "        (conv1d): Conv1d(8192, 8192, kernel_size=(4,), stride=(1,), padding=(3,), groups=8192)\n",
       "        (act): SiLU()\n",
       "        (in_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "        (x_proj): Linear(in_features=8192, out_features=288, bias=False)\n",
       "        (dt_proj): Linear(in_features=256, out_features=8192, bias=True)\n",
       "        (out_proj): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "        (dt_layernorm): JambaRMSNorm((256,), eps=1e-06)\n",
       "        (b_layernorm): JambaRMSNorm((16,), eps=1e-06)\n",
       "        (c_layernorm): JambaRMSNorm((16,), eps=1e-06)\n",
       "      )\n",
       "      (feed_forward): JambaMLP(\n",
       "        (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "        (act_fn): SiLU()\n",
       "      )\n",
       "      (input_layernorm): JambaRMSNorm((4096,), eps=1e-06)\n",
       "      (pre_ff_layernorm): JambaRMSNorm((4096,), eps=1e-06)\n",
       "    )\n",
       "    (1): JambaAttentionDecoderLayer(\n",
       "      (self_attn): JambaSdpaAttention(\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (feed_forward): JambaSparseMoeBlock(\n",
       "        (router): Linear(in_features=4096, out_features=2, bias=False)\n",
       "        (experts): ModuleList(\n",
       "          (0-1): 2 x JambaMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (input_layernorm): JambaRMSNorm((4096,), eps=1e-06)\n",
       "      (pre_ff_layernorm): JambaRMSNorm((4096,), eps=1e-06)\n",
       "    )\n",
       "    (2): JambaMambaDecoderLayer(\n",
       "      (mamba): JambaMambaMixer(\n",
       "        (conv1d): Conv1d(8192, 8192, kernel_size=(4,), stride=(1,), padding=(3,), groups=8192)\n",
       "        (act): SiLU()\n",
       "        (in_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "        (x_proj): Linear(in_features=8192, out_features=288, bias=False)\n",
       "        (dt_proj): Linear(in_features=256, out_features=8192, bias=True)\n",
       "        (out_proj): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "        (dt_layernorm): JambaRMSNorm((256,), eps=1e-06)\n",
       "        (b_layernorm): JambaRMSNorm((16,), eps=1e-06)\n",
       "        (c_layernorm): JambaRMSNorm((16,), eps=1e-06)\n",
       "      )\n",
       "      (feed_forward): JambaMLP(\n",
       "        (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "        (act_fn): SiLU()\n",
       "      )\n",
       "      (input_layernorm): JambaRMSNorm((4096,), eps=1e-06)\n",
       "      (pre_ff_layernorm): JambaRMSNorm((4096,), eps=1e-06)\n",
       "    )\n",
       "    (3): JambaMambaDecoderLayer(\n",
       "      (mamba): JambaMambaMixer(\n",
       "        (conv1d): Conv1d(8192, 8192, kernel_size=(4,), stride=(1,), padding=(3,), groups=8192)\n",
       "        (act): SiLU()\n",
       "        (in_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "        (x_proj): Linear(in_features=8192, out_features=288, bias=False)\n",
       "        (dt_proj): Linear(in_features=256, out_features=8192, bias=True)\n",
       "        (out_proj): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "        (dt_layernorm): JambaRMSNorm((256,), eps=1e-06)\n",
       "        (b_layernorm): JambaRMSNorm((16,), eps=1e-06)\n",
       "        (c_layernorm): JambaRMSNorm((16,), eps=1e-06)\n",
       "      )\n",
       "      (feed_forward): JambaSparseMoeBlock(\n",
       "        (router): Linear(in_features=4096, out_features=2, bias=False)\n",
       "        (experts): ModuleList(\n",
       "          (0-1): 2 x JambaMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (input_layernorm): JambaRMSNorm((4096,), eps=1e-06)\n",
       "      (pre_ff_layernorm): JambaRMSNorm((4096,), eps=1e-06)\n",
       "    )\n",
       "    (4): JambaMambaDecoderLayer(\n",
       "      (mamba): JambaMambaMixer(\n",
       "        (conv1d): Conv1d(8192, 8192, kernel_size=(4,), stride=(1,), padding=(3,), groups=8192)\n",
       "        (act): SiLU()\n",
       "        (in_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "        (x_proj): Linear(in_features=8192, out_features=288, bias=False)\n",
       "        (dt_proj): Linear(in_features=256, out_features=8192, bias=True)\n",
       "        (out_proj): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "        (dt_layernorm): JambaRMSNorm((256,), eps=1e-06)\n",
       "        (b_layernorm): JambaRMSNorm((16,), eps=1e-06)\n",
       "        (c_layernorm): JambaRMSNorm((16,), eps=1e-06)\n",
       "      )\n",
       "      (feed_forward): JambaMLP(\n",
       "        (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "        (act_fn): SiLU()\n",
       "      )\n",
       "      (input_layernorm): JambaRMSNorm((4096,), eps=1e-06)\n",
       "      (pre_ff_layernorm): JambaRMSNorm((4096,), eps=1e-06)\n",
       "    )\n",
       "    (5): JambaMambaDecoderLayer(\n",
       "      (mamba): JambaMambaMixer(\n",
       "        (conv1d): Conv1d(8192, 8192, kernel_size=(4,), stride=(1,), padding=(3,), groups=8192)\n",
       "        (act): SiLU()\n",
       "        (in_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "        (x_proj): Linear(in_features=8192, out_features=288, bias=False)\n",
       "        (dt_proj): Linear(in_features=256, out_features=8192, bias=True)\n",
       "        (out_proj): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "        (dt_layernorm): JambaRMSNorm((256,), eps=1e-06)\n",
       "        (b_layernorm): JambaRMSNorm((16,), eps=1e-06)\n",
       "        (c_layernorm): JambaRMSNorm((16,), eps=1e-06)\n",
       "      )\n",
       "      (feed_forward): JambaSparseMoeBlock(\n",
       "        (router): Linear(in_features=4096, out_features=2, bias=False)\n",
       "        (experts): ModuleList(\n",
       "          (0-1): 2 x JambaMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (input_layernorm): JambaRMSNorm((4096,), eps=1e-06)\n",
       "      (pre_ff_layernorm): JambaRMSNorm((4096,), eps=1e-06)\n",
       "    )\n",
       "    (6): JambaMambaDecoderLayer(\n",
       "      (mamba): JambaMambaMixer(\n",
       "        (conv1d): Conv1d(8192, 8192, kernel_size=(4,), stride=(1,), padding=(3,), groups=8192)\n",
       "        (act): SiLU()\n",
       "        (in_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "        (x_proj): Linear(in_features=8192, out_features=288, bias=False)\n",
       "        (dt_proj): Linear(in_features=256, out_features=8192, bias=True)\n",
       "        (out_proj): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "        (dt_layernorm): JambaRMSNorm((256,), eps=1e-06)\n",
       "        (b_layernorm): JambaRMSNorm((16,), eps=1e-06)\n",
       "        (c_layernorm): JambaRMSNorm((16,), eps=1e-06)\n",
       "      )\n",
       "      (feed_forward): JambaMLP(\n",
       "        (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "        (act_fn): SiLU()\n",
       "      )\n",
       "      (input_layernorm): JambaRMSNorm((4096,), eps=1e-06)\n",
       "      (pre_ff_layernorm): JambaRMSNorm((4096,), eps=1e-06)\n",
       "    )\n",
       "    (7): JambaMambaDecoderLayer(\n",
       "      (mamba): JambaMambaMixer(\n",
       "        (conv1d): Conv1d(8192, 8192, kernel_size=(4,), stride=(1,), padding=(3,), groups=8192)\n",
       "        (act): SiLU()\n",
       "        (in_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "        (x_proj): Linear(in_features=8192, out_features=288, bias=False)\n",
       "        (dt_proj): Linear(in_features=256, out_features=8192, bias=True)\n",
       "        (out_proj): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "        (dt_layernorm): JambaRMSNorm((256,), eps=1e-06)\n",
       "        (b_layernorm): JambaRMSNorm((16,), eps=1e-06)\n",
       "        (c_layernorm): JambaRMSNorm((16,), eps=1e-06)\n",
       "      )\n",
       "      (feed_forward): JambaSparseMoeBlock(\n",
       "        (router): Linear(in_features=4096, out_features=2, bias=False)\n",
       "        (experts): ModuleList(\n",
       "          (0-1): 2 x JambaMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (input_layernorm): JambaRMSNorm((4096,), eps=1e-06)\n",
       "      (pre_ff_layernorm): JambaRMSNorm((4096,), eps=1e-06)\n",
       "    )\n",
       "  )\n",
       "  (final_layernorm): JambaRMSNorm((4096,), eps=1e-06)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # standard jamba config\n",
    "# from transformers import JambaConfig, JambaModel\n",
    "# jamba_config = {\n",
    "#     \"vocab_size\": 65536,\n",
    "#     \"hidden_size\": 4096,\n",
    "#     \"intermediate_size\": 14336,\n",
    "#     \"num_hidden_layers\": 8,\n",
    "#     \"num_attention_heads\": 32,\n",
    "#     \"num_key_value_heads\": 8,\n",
    "\n",
    "#     # Expert / MoE (optional)\n",
    "#     \"num_experts_per_tok\": 2,\n",
    "#     \"num_experts\": 2,\n",
    "#     \"expert_layer_offset\": 1,\n",
    "\n",
    "#     # Attention layer config\n",
    "#     \"attn_layer_period\": 8,\n",
    "#     \"attn_layer_offset\": 1,\n",
    "\n",
    "#     # Mamba-specific config\n",
    "#     \"use_mamba_kernels\": True,\n",
    "#     \"mamba_d_state\": 16,\n",
    "#     \"mamba_d_conv\": 4,\n",
    "#     \"mamba_expand\": 2,\n",
    "#     \"mamba_dt_rank\": \"auto\",\n",
    "#     \"mamba_conv_bias\": True,\n",
    "#     \"mamba_proj_bias\": False,\n",
    "# }\n",
    "\n",
    "# config = JambaConfig(**jamba_config)\n",
    "# model = JambaModel(config)\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c00f68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "accuracy_metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "458d4cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy_metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2581333e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b46d6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size on GPU: 193.35 MB\n"
     ]
    }
   ],
   "source": [
    "def get_model_size_in_mb(model):\n",
    "    param_size = 0\n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "        \n",
    "    buffer_size = 0\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "        \n",
    "    total_size = param_size + buffer_size  # in bytes\n",
    "    return total_size / (1024 ** 2)  # convert to MB\n",
    "\n",
    "\n",
    "size_mb = get_model_size_in_mb(model)\n",
    "print(f\"Model size on GPU: {size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa981595",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tensor_size_in_mb(tensor):\n",
    "    return tensor.nelement() * tensor.element_size() / (1024 ** 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65313de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95a294fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Jamba requires an initialized `HybridMambaAttentionDynamicCache` to return a cache. None was provided, so no cache will be returned.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutputWithPast(loss=tensor(0.7621, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2880, -0.7416],\n",
      "        [ 0.2785, -0.7359],\n",
      "        [ 0.2454, -0.6716],\n",
      "        [ 0.2198, -0.7384],\n",
      "        [ 0.1910, -0.7401],\n",
      "        [ 0.0114, -0.4809],\n",
      "        [ 0.0415,  0.4010],\n",
      "        [ 0.0848, -0.5266]], device='cuda:0', grad_fn=<IndexBackward0>), past_key_values=None, hidden_states=None, attentions=None)\n",
      "Time taken for batch:  20.169976472854614\n",
      "SequenceClassifierOutputWithPast(loss=tensor(2.9234, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-2.5542,  2.6499],\n",
      "        [ 0.0822,  0.1846],\n",
      "        [-2.7989,  2.4625],\n",
      "        [ 0.2692,  0.4268],\n",
      "        [-0.7409, -0.2041],\n",
      "        [ 0.0714,  0.1226],\n",
      "        [-2.5638,  2.6394],\n",
      "        [-2.5653,  2.6405]], device='cuda:0', grad_fn=<IndexBackward0>), past_key_values=None, hidden_states=None, attentions=None)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Backward pass and optimize\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     28\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "model.to(device)\n",
    "# Loss function and optimizer\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    model.train() \n",
    "    total_loss = 0\n",
    "    len_data = 0\n",
    "    for batch in train_dataloader:\n",
    "        start_time = time.time()\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        inputs = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        targets = batch['labels'].to(device)\n",
    "        #print(\"Data_size\", get_tensor_size_in_mb(inputs)+ get_tensor_size_in_mb(attention_mask) + get_tensor_size_in_mb(targets))\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids = inputs, attention_mask=attention_mask,labels = targets) \n",
    "        print(outputs)\n",
    "        loss = outputs.loss\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        end_time = time.time()\n",
    "        total_loss += loss\n",
    "        print(\"Time taken for batch: \", end_time - start_time)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_dataloader)}\")\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2d150c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71b24d64",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error initializing torch.distributed using env:// rendezvous: environment variable RANK expected, but not set",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdevice_mesh\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m init_device_mesh\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# creating a device mesh that connnets 8 GPUs in a host\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m tp_mesh \u001b[38;5;241m=\u001b[39m \u001b[43minit_device_mesh\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/distributed/device_mesh.py:713\u001b[0m, in \u001b[0;36minit_device_mesh\u001b[0;34m(device_type, mesh_shape, mesh_dim_names)\u001b[0m\n\u001b[1;32m    711\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    712\u001b[0m     mesh \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(math\u001b[38;5;241m.\u001b[39mprod(mesh_shape), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint)\u001b[38;5;241m.\u001b[39mview(mesh_shape)\n\u001b[0;32m--> 713\u001b[0m device_mesh \u001b[38;5;241m=\u001b[39m \u001b[43mDeviceMesh\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmesh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmesh_dim_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmesh_dim_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m device_mesh\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/distributed/device_mesh.py:255\u001b[0m, in \u001b[0;36mDeviceMesh.__init__\u001b[0;34m(self, device_type, mesh, mesh_dim_names, _init_backend)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device_type \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;66;03m# always try to create default (world) pg, even if it is not initialized\u001b[39;00m\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;66;03m# already. The world pg is used for device mesh identity (rank) on each\u001b[39;00m\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;66;03m# process (we need to know if the current global rank is in the mesh or not).\u001b[39;00m\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _init_backend:\n\u001b[0;32m--> 255\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_or_create_default_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_process_groups()\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;66;03m# calculate the coordinates of the current global rank on the mesh\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/distributed/device_mesh.py:268\u001b[0m, in \u001b[0;36mDeviceMesh._get_or_create_default_group\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    266\u001b[0m default_initialized \u001b[38;5;241m=\u001b[39m is_initialized()\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m default_initialized:\n\u001b[0;32m--> 268\u001b[0m     \u001b[43minit_process_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m world_size \u001b[38;5;241m=\u001b[39m get_world_size()\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmesh\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m>\u001b[39m world_size:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/distributed/c10d_logger.py:79\u001b[0m, in \u001b[0;36m_exception_logger.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: _P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: _P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _T:\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 79\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[1;32m     81\u001b[0m         msg_dict \u001b[38;5;241m=\u001b[39m _get_msg_dict(func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/distributed/c10d_logger.py:93\u001b[0m, in \u001b[0;36m_time_logger.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: _P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: _P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _T:\n\u001b[1;32m     92\u001b[0m     t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime_ns()\n\u001b[0;32m---> 93\u001b[0m     func_return \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m     time_spent \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime_ns() \u001b[38;5;241m-\u001b[39m t1\n\u001b[1;32m     96\u001b[0m     msg_dict \u001b[38;5;241m=\u001b[39m _get_msg_dict(func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:1361\u001b[0m, in \u001b[0;36minit_process_group\u001b[0;34m(backend, init_method, timeout, world_size, rank, store, group_name, pg_options, device_id)\u001b[0m\n\u001b[1;32m   1357\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m store \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1358\u001b[0m     rendezvous_iterator \u001b[38;5;241m=\u001b[39m rendezvous(\n\u001b[1;32m   1359\u001b[0m         not_none(init_method), rank, world_size, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m   1360\u001b[0m     )\n\u001b[0;32m-> 1361\u001b[0m     store, rank, world_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrendezvous_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1362\u001b[0m     store\u001b[38;5;241m.\u001b[39mset_timeout(timeout)\n\u001b[1;32m   1364\u001b[0m     \u001b[38;5;66;03m# Use a PrefixStore to avoid accidental overrides of keys used by\u001b[39;00m\n\u001b[1;32m   1365\u001b[0m     \u001b[38;5;66;03m# different systems (e.g. RPC) in case the store is multi-tenant.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/distributed/rendezvous.py:246\u001b[0m, in \u001b[0;36m_env_rendezvous_handler\u001b[0;34m(url, timeout, **kwargs)\u001b[0m\n\u001b[1;32m    244\u001b[0m     rank \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(query_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrank\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 246\u001b[0m     rank \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[43m_get_env_or_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRANK\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mworld_size\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m query_dict:\n\u001b[1;32m    249\u001b[0m     world_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(query_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mworld_size\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/distributed/rendezvous.py:231\u001b[0m, in \u001b[0;36m_env_rendezvous_handler.<locals>._get_env_or_raise\u001b[0;34m(env_var)\u001b[0m\n\u001b[1;32m    229\u001b[0m env_val \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(env_var, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m env_val:\n\u001b[0;32m--> 231\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _env_error(env_var)\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_val\n",
      "\u001b[0;31mValueError\u001b[0m: Error initializing torch.distributed using env:// rendezvous: environment variable RANK expected, but not set"
     ]
    }
   ],
   "source": [
    "from torch.distributed.device_mesh import init_device_mesh\n",
    "\n",
    "# creating a device mesh that connnets 8 GPUs in a host\n",
    "tp_mesh = init_device_mesh(\"cuda\",(8,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2dc32e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2771961923.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[9], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    https://stackoverflow.com/questions/56805951/valueerror-error-initializing-torch-distributed-using-env-rendezvous-enviro\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\"https://stackoverflow.com/questions/56805951/valueerror-error-initializing-torch-distributed-using-env-rendezvous-enviro\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38c2595f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99cd84de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20250421-162858'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.now().strftime(\"%Y%m%d-%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95156471",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
