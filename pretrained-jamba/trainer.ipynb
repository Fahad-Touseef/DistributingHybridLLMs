{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "375116b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, JambaForSequenceClassification, JambaConfig\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32910fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the IMDB dataset from Hugging Face\n",
    "imdb = load_dataset(\"imdb\")\n",
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ai21labs/Jamba-v0.1\")\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)  # padding left to collator\n",
    "tokenized_imdb = imdb.map(preprocess_function, batched=True)\n",
    "\n",
    "tokenized_imdb = tokenized_imdb.remove_columns([\"text\"])\n",
    "tokenized_imdb.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "#outputs = model.generate(input_ids, max_new_tokens=216)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6bc2143",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    tokenized_imdb[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=8,\n",
    "    collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d83002f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
    "label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0770ec6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,     0,     0,  ...,  4007,  2908, 62959],\n",
       "        [    0,     0,     0,  ...,  1808,  2686, 62959],\n",
       "        [    0,     0,     0,  ..., 63009,  3910, 62959],\n",
       "        ...,\n",
       "        [    1,  3921,  2100,  ..., 19953,  2482, 25691],\n",
       "        [    0,     0,     0,  ...,  6868,  1895, 62959],\n",
       "        [    0,     0,     0,  ...,  1831, 17135, 62959]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
       "        [0, 0, 0,  ..., 1, 1, 1],\n",
       "        [0, 0, 0,  ..., 1, 1, 1],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [0, 0, 0,  ..., 1, 1, 1],\n",
       "        [0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([0, 0, 0, 0, 1, 0, 0, 1])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3508390f",
   "metadata": {},
   "outputs": [],
   "source": [
    "jamba_config = {\n",
    "    \"vocab_size\": len(tokenizer.vocab),\n",
    "    \"hidden_size\": 256,\n",
    "    \"intermediate_size\": 14336,\n",
    "    \"num_hidden_layers\": 2,\n",
    "    \"num_attention_heads\": 4,\n",
    "    \"num_key_value_heads\": 4,\n",
    "\n",
    "    # Expert / MoE (optional)\n",
    "    \"num_experts_per_tok\": 1,\n",
    "    \"num_experts\": 2,\n",
    "    \"expert_layer_offset\": 1,\n",
    "\n",
    "    # Attention layer config\n",
    "    \"attn_layer_period\": 1,\n",
    "    \"attn_layer_offset\": 1,\n",
    "\n",
    "    # Mamba-specific config\n",
    "    \"use_mamba_kernels\": False,\n",
    "    \"mamba_d_state\": 16,\n",
    "    \"mamba_d_conv\": 4,\n",
    "    \"mamba_expand\": 2,\n",
    "}\n",
    "# config = JambaConfig(\n",
    "#     vocab_size=50257,\n",
    "#     dim=512,\n",
    "#     num_hidden_layers=8, # number of hidden layers in transformer block\n",
    "\n",
    "#     hidden_size=256,\n",
    "#     attn_layer_period = 8, # every 4 layer there is a attention layer\n",
    "#     attention_offset = 0, # offset for attention layer\n",
    "#     num_attention_heads=8,\n",
    "#     num_key_value_heads = 8, # if equal with num_attention_heads, then use MultiheadAttention\n",
    "\n",
    "#     d_conv=4,\n",
    "#     d_state=256,\n",
    "#     num_experts_per_tok = 2,  # a router choosing 2 experts per token\n",
    "#     num_experts=2, # total number of experts\n",
    "#     expert_layer_period =4, # every 4 layer there is a expert layer\n",
    "# )\n",
    "config = JambaConfig(**jamba_config)\n",
    "model  = JambaForSequenceClassification(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16c7fd58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JambaForSequenceClassification(\n",
       "  (model): JambaModel(\n",
       "    (embed_tokens): Embedding(65536, 256, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0): JambaMambaDecoderLayer(\n",
       "        (mamba): JambaMambaMixer(\n",
       "          (conv1d): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)\n",
       "          (act): SiLU()\n",
       "          (in_proj): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (x_proj): Linear(in_features=512, out_features=48, bias=False)\n",
       "          (dt_proj): Linear(in_features=16, out_features=512, bias=True)\n",
       "          (out_proj): Linear(in_features=512, out_features=256, bias=False)\n",
       "          (dt_layernorm): JambaRMSNorm((16,), eps=1e-06)\n",
       "          (b_layernorm): JambaRMSNorm((16,), eps=1e-06)\n",
       "          (c_layernorm): JambaRMSNorm((16,), eps=1e-06)\n",
       "        )\n",
       "        (feed_forward): JambaMLP(\n",
       "          (gate_proj): Linear(in_features=256, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=256, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=256, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): JambaRMSNorm((256,), eps=1e-06)\n",
       "        (pre_ff_layernorm): JambaRMSNorm((256,), eps=1e-06)\n",
       "      )\n",
       "      (1): JambaMambaDecoderLayer(\n",
       "        (mamba): JambaMambaMixer(\n",
       "          (conv1d): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)\n",
       "          (act): SiLU()\n",
       "          (in_proj): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (x_proj): Linear(in_features=512, out_features=48, bias=False)\n",
       "          (dt_proj): Linear(in_features=16, out_features=512, bias=True)\n",
       "          (out_proj): Linear(in_features=512, out_features=256, bias=False)\n",
       "          (dt_layernorm): JambaRMSNorm((16,), eps=1e-06)\n",
       "          (b_layernorm): JambaRMSNorm((16,), eps=1e-06)\n",
       "          (c_layernorm): JambaRMSNorm((16,), eps=1e-06)\n",
       "        )\n",
       "        (feed_forward): JambaSparseMoeBlock(\n",
       "          (router): Linear(in_features=256, out_features=2, bias=False)\n",
       "          (experts): ModuleList(\n",
       "            (0-1): 2 x JambaMLP(\n",
       "              (gate_proj): Linear(in_features=256, out_features=14336, bias=False)\n",
       "              (up_proj): Linear(in_features=256, out_features=14336, bias=False)\n",
       "              (down_proj): Linear(in_features=14336, out_features=256, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): JambaRMSNorm((256,), eps=1e-06)\n",
       "        (pre_ff_layernorm): JambaRMSNorm((256,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (final_layernorm): JambaRMSNorm((256,), eps=1e-06)\n",
       "  )\n",
       "  (score): Linear(in_features=256, out_features=2, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cc60d2",
   "metadata": {},
   "source": [
    "### Standard Jamba Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77768665",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JambaModel(\n",
       "  (embed_tokens): Embedding(65536, 4096, padding_idx=0)\n",
       "  (layers): ModuleList(\n",
       "    (0): JambaMambaDecoderLayer(\n",
       "      (mamba): JambaMambaMixer(\n",
       "        (conv1d): Conv1d(8192, 8192, kernel_size=(4,), stride=(1,), padding=(3,), groups=8192)\n",
       "        (act): SiLU()\n",
       "        (in_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "        (x_proj): Linear(in_features=8192, out_features=288, bias=False)\n",
       "        (dt_proj): Linear(in_features=256, out_features=8192, bias=True)\n",
       "        (out_proj): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "        (dt_layernorm): JambaRMSNorm((256,), eps=1e-06)\n",
       "        (b_layernorm): JambaRMSNorm((16,), eps=1e-06)\n",
       "        (c_layernorm): JambaRMSNorm((16,), eps=1e-06)\n",
       "      )\n",
       "      (feed_forward): JambaMLP(\n",
       "        (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "        (act_fn): SiLU()\n",
       "      )\n",
       "      (input_layernorm): JambaRMSNorm((4096,), eps=1e-06)\n",
       "      (pre_ff_layernorm): JambaRMSNorm((4096,), eps=1e-06)\n",
       "    )\n",
       "    (1): JambaAttentionDecoderLayer(\n",
       "      (self_attn): JambaSdpaAttention(\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (feed_forward): JambaSparseMoeBlock(\n",
       "        (router): Linear(in_features=4096, out_features=2, bias=False)\n",
       "        (experts): ModuleList(\n",
       "          (0-1): 2 x JambaMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (input_layernorm): JambaRMSNorm((4096,), eps=1e-06)\n",
       "      (pre_ff_layernorm): JambaRMSNorm((4096,), eps=1e-06)\n",
       "    )\n",
       "    (2): JambaMambaDecoderLayer(\n",
       "      (mamba): JambaMambaMixer(\n",
       "        (conv1d): Conv1d(8192, 8192, kernel_size=(4,), stride=(1,), padding=(3,), groups=8192)\n",
       "        (act): SiLU()\n",
       "        (in_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "        (x_proj): Linear(in_features=8192, out_features=288, bias=False)\n",
       "        (dt_proj): Linear(in_features=256, out_features=8192, bias=True)\n",
       "        (out_proj): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "        (dt_layernorm): JambaRMSNorm((256,), eps=1e-06)\n",
       "        (b_layernorm): JambaRMSNorm((16,), eps=1e-06)\n",
       "        (c_layernorm): JambaRMSNorm((16,), eps=1e-06)\n",
       "      )\n",
       "      (feed_forward): JambaMLP(\n",
       "        (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "        (act_fn): SiLU()\n",
       "      )\n",
       "      (input_layernorm): JambaRMSNorm((4096,), eps=1e-06)\n",
       "      (pre_ff_layernorm): JambaRMSNorm((4096,), eps=1e-06)\n",
       "    )\n",
       "    (3): JambaMambaDecoderLayer(\n",
       "      (mamba): JambaMambaMixer(\n",
       "        (conv1d): Conv1d(8192, 8192, kernel_size=(4,), stride=(1,), padding=(3,), groups=8192)\n",
       "        (act): SiLU()\n",
       "        (in_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "        (x_proj): Linear(in_features=8192, out_features=288, bias=False)\n",
       "        (dt_proj): Linear(in_features=256, out_features=8192, bias=True)\n",
       "        (out_proj): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "        (dt_layernorm): JambaRMSNorm((256,), eps=1e-06)\n",
       "        (b_layernorm): JambaRMSNorm((16,), eps=1e-06)\n",
       "        (c_layernorm): JambaRMSNorm((16,), eps=1e-06)\n",
       "      )\n",
       "      (feed_forward): JambaSparseMoeBlock(\n",
       "        (router): Linear(in_features=4096, out_features=2, bias=False)\n",
       "        (experts): ModuleList(\n",
       "          (0-1): 2 x JambaMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (input_layernorm): JambaRMSNorm((4096,), eps=1e-06)\n",
       "      (pre_ff_layernorm): JambaRMSNorm((4096,), eps=1e-06)\n",
       "    )\n",
       "    (4): JambaMambaDecoderLayer(\n",
       "      (mamba): JambaMambaMixer(\n",
       "        (conv1d): Conv1d(8192, 8192, kernel_size=(4,), stride=(1,), padding=(3,), groups=8192)\n",
       "        (act): SiLU()\n",
       "        (in_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "        (x_proj): Linear(in_features=8192, out_features=288, bias=False)\n",
       "        (dt_proj): Linear(in_features=256, out_features=8192, bias=True)\n",
       "        (out_proj): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "        (dt_layernorm): JambaRMSNorm((256,), eps=1e-06)\n",
       "        (b_layernorm): JambaRMSNorm((16,), eps=1e-06)\n",
       "        (c_layernorm): JambaRMSNorm((16,), eps=1e-06)\n",
       "      )\n",
       "      (feed_forward): JambaMLP(\n",
       "        (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "        (act_fn): SiLU()\n",
       "      )\n",
       "      (input_layernorm): JambaRMSNorm((4096,), eps=1e-06)\n",
       "      (pre_ff_layernorm): JambaRMSNorm((4096,), eps=1e-06)\n",
       "    )\n",
       "    (5): JambaMambaDecoderLayer(\n",
       "      (mamba): JambaMambaMixer(\n",
       "        (conv1d): Conv1d(8192, 8192, kernel_size=(4,), stride=(1,), padding=(3,), groups=8192)\n",
       "        (act): SiLU()\n",
       "        (in_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "        (x_proj): Linear(in_features=8192, out_features=288, bias=False)\n",
       "        (dt_proj): Linear(in_features=256, out_features=8192, bias=True)\n",
       "        (out_proj): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "        (dt_layernorm): JambaRMSNorm((256,), eps=1e-06)\n",
       "        (b_layernorm): JambaRMSNorm((16,), eps=1e-06)\n",
       "        (c_layernorm): JambaRMSNorm((16,), eps=1e-06)\n",
       "      )\n",
       "      (feed_forward): JambaSparseMoeBlock(\n",
       "        (router): Linear(in_features=4096, out_features=2, bias=False)\n",
       "        (experts): ModuleList(\n",
       "          (0-1): 2 x JambaMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (input_layernorm): JambaRMSNorm((4096,), eps=1e-06)\n",
       "      (pre_ff_layernorm): JambaRMSNorm((4096,), eps=1e-06)\n",
       "    )\n",
       "    (6): JambaMambaDecoderLayer(\n",
       "      (mamba): JambaMambaMixer(\n",
       "        (conv1d): Conv1d(8192, 8192, kernel_size=(4,), stride=(1,), padding=(3,), groups=8192)\n",
       "        (act): SiLU()\n",
       "        (in_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "        (x_proj): Linear(in_features=8192, out_features=288, bias=False)\n",
       "        (dt_proj): Linear(in_features=256, out_features=8192, bias=True)\n",
       "        (out_proj): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "        (dt_layernorm): JambaRMSNorm((256,), eps=1e-06)\n",
       "        (b_layernorm): JambaRMSNorm((16,), eps=1e-06)\n",
       "        (c_layernorm): JambaRMSNorm((16,), eps=1e-06)\n",
       "      )\n",
       "      (feed_forward): JambaMLP(\n",
       "        (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "        (act_fn): SiLU()\n",
       "      )\n",
       "      (input_layernorm): JambaRMSNorm((4096,), eps=1e-06)\n",
       "      (pre_ff_layernorm): JambaRMSNorm((4096,), eps=1e-06)\n",
       "    )\n",
       "    (7): JambaMambaDecoderLayer(\n",
       "      (mamba): JambaMambaMixer(\n",
       "        (conv1d): Conv1d(8192, 8192, kernel_size=(4,), stride=(1,), padding=(3,), groups=8192)\n",
       "        (act): SiLU()\n",
       "        (in_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "        (x_proj): Linear(in_features=8192, out_features=288, bias=False)\n",
       "        (dt_proj): Linear(in_features=256, out_features=8192, bias=True)\n",
       "        (out_proj): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "        (dt_layernorm): JambaRMSNorm((256,), eps=1e-06)\n",
       "        (b_layernorm): JambaRMSNorm((16,), eps=1e-06)\n",
       "        (c_layernorm): JambaRMSNorm((16,), eps=1e-06)\n",
       "      )\n",
       "      (feed_forward): JambaSparseMoeBlock(\n",
       "        (router): Linear(in_features=4096, out_features=2, bias=False)\n",
       "        (experts): ModuleList(\n",
       "          (0-1): 2 x JambaMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (input_layernorm): JambaRMSNorm((4096,), eps=1e-06)\n",
       "      (pre_ff_layernorm): JambaRMSNorm((4096,), eps=1e-06)\n",
       "    )\n",
       "  )\n",
       "  (final_layernorm): JambaRMSNorm((4096,), eps=1e-06)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # standard jamba config\n",
    "# from transformers import JambaConfig, JambaModel\n",
    "# jamba_config = {\n",
    "#     \"vocab_size\": 65536,\n",
    "#     \"hidden_size\": 4096,\n",
    "#     \"intermediate_size\": 14336,\n",
    "#     \"num_hidden_layers\": 8,\n",
    "#     \"num_attention_heads\": 32,\n",
    "#     \"num_key_value_heads\": 8,\n",
    "\n",
    "#     # Expert / MoE (optional)\n",
    "#     \"num_experts_per_tok\": 2,\n",
    "#     \"num_experts\": 2,\n",
    "#     \"expert_layer_offset\": 1,\n",
    "\n",
    "#     # Attention layer config\n",
    "#     \"attn_layer_period\": 8,\n",
    "#     \"attn_layer_offset\": 1,\n",
    "\n",
    "#     # Mamba-specific config\n",
    "#     \"use_mamba_kernels\": True,\n",
    "#     \"mamba_d_state\": 16,\n",
    "#     \"mamba_d_conv\": 4,\n",
    "#     \"mamba_expand\": 2,\n",
    "#     \"mamba_dt_rank\": \"auto\",\n",
    "#     \"mamba_conv_bias\": True,\n",
    "#     \"mamba_proj_bias\": False,\n",
    "# }\n",
    "\n",
    "# config = JambaConfig(**jamba_config)\n",
    "# model = JambaModel(config)\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c00f68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "accuracy_metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "458d4cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy_metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2581333e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b46d6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size on GPU: 193.35 MB\n"
     ]
    }
   ],
   "source": [
    "def get_model_size_in_mb(model):\n",
    "    param_size = 0\n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "        \n",
    "    buffer_size = 0\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "        \n",
    "    total_size = param_size + buffer_size  # in bytes\n",
    "    return total_size / (1024 ** 2)  # convert to MB\n",
    "\n",
    "\n",
    "size_mb = get_model_size_in_mb(model)\n",
    "print(f\"Model size on GPU: {size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa981595",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tensor_size_in_mb(tensor):\n",
    "    return tensor.nelement() * tensor.element_size() / (1024 ** 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65313de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a294fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data_size 0.07501220703125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Jamba requires an initialized `HybridMambaAttentionDynamicCache` to return a cache. None was provided, so no cache will be returned.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for batch:  20.96748375892639\n",
      "Data_size 0.12152099609375\n",
      "Time taken for batch:  19.315913438796997\n",
      "Data_size 0.07318115234375\n",
      "Time taken for batch:  7.181234836578369\n",
      "Data_size 0.03680419921875\n",
      "Time taken for batch:  1.962125539779663\n",
      "Data_size 0.06378173828125\n",
      "Time taken for batch:  5.347221374511719\n",
      "Data_size 0.06329345703125\n",
      "Time taken for batch:  5.368344306945801\n",
      "Data_size 0.14129638671875\n",
      "Time taken for batch:  26.02852749824524\n",
      "Data_size 0.09588623046875\n",
      "Time taken for batch:  12.141685009002686\n",
      "Data_size 0.12298583984375\n",
      "Time taken for batch:  19.72804594039917\n",
      "Data_size 0.07293701171875\n",
      "Time taken for batch:  7.188098907470703\n",
      "Data_size 0.04217529296875\n",
      "Time taken for batch:  2.4103729724884033\n",
      "Data_size 0.06085205078125\n",
      "Time taken for batch:  4.853060245513916\n",
      "Data_size 0.08587646484375\n",
      "Time taken for batch:  9.708363056182861\n",
      "Data_size 0.08660888671875\n",
      "Time taken for batch:  9.91413140296936\n",
      "Data_size 0.08880615234375\n",
      "Time taken for batch:  10.418190479278564\n",
      "Data_size 0.05975341796875\n",
      "Time taken for batch:  4.85733437538147\n",
      "Data_size 0.15802001953125\n",
      "Time taken for batch:  32.46296977996826\n",
      "Data_size 0.08514404296875\n",
      "Time taken for batch:  9.727385997772217\n",
      "Data_size 0.08966064453125\n",
      "Time taken for batch:  10.614749431610107\n",
      "Data_size 0.04461669921875\n",
      "Time taken for batch:  2.812652349472046\n",
      "Data_size 0.08538818359375\n",
      "Time taken for batch:  9.574433088302612\n",
      "Data_size 0.09747314453125\n",
      "Time taken for batch:  12.495049238204956\n",
      "Data_size 0.10858154296875\n",
      "Time taken for batch:  15.45301342010498\n",
      "Data_size 0.14202880859375\n",
      "Time taken for batch:  26.215367317199707\n",
      "Data_size 0.11773681640625\n",
      "Time taken for batch:  18.18951439857483\n",
      "Data_size 0.05450439453125\n",
      "Time taken for batch:  4.145177364349365\n",
      "Data_size 0.07672119140625\n",
      "Time taken for batch:  7.776512622833252\n",
      "Data_size 0.07061767578125\n",
      "Time taken for batch:  6.6603522300720215\n",
      "Data_size 0.07550048828125\n",
      "Time taken for batch:  7.569297790527344\n",
      "Data_size 0.06683349609375\n",
      "Time taken for batch:  5.986572265625\n",
      "Data_size 0.08917236328125\n",
      "Time taken for batch:  10.467633485794067\n",
      "Data_size 0.03900146484375\n",
      "Time taken for batch:  2.205498456954956\n",
      "Data_size 0.09112548828125\n",
      "Time taken for batch:  10.879223108291626\n",
      "Data_size 0.10626220703125\n",
      "Time taken for batch:  14.806298971176147\n",
      "Data_size 0.08819580078125\n",
      "Time taken for batch:  10.313437700271606\n",
      "Data_size 0.09686279296875\n",
      "Time taken for batch:  12.534017562866211\n",
      "Data_size 0.11224365234375\n",
      "Time taken for batch:  16.495356798171997\n",
      "Data_size 0.11346435546875\n",
      "Time taken for batch:  16.87303900718689\n",
      "Data_size 0.07977294921875\n",
      "Time taken for batch:  8.515562295913696\n",
      "Data_size 0.05755615234375\n",
      "Time taken for batch:  4.5149688720703125\n",
      "Data_size 0.04266357421875\n",
      "Time taken for batch:  2.529630184173584\n",
      "Data_size 0.12591552734375\n",
      "Time taken for batch:  20.55422043800354\n",
      "Data_size 0.07342529296875\n",
      "Time taken for batch:  7.280011177062988\n",
      "Data_size 0.16717529296875\n",
      "Time taken for batch:  36.42700958251953\n",
      "Data_size 0.12078857421875\n",
      "Time taken for batch:  19.185043334960938\n",
      "Data_size 0.07659912109375\n",
      "Time taken for batch:  7.900466680526733\n",
      "Data_size 0.15850830078125\n",
      "Time taken for batch:  32.53841018676758\n",
      "Data_size 0.05706787109375\n",
      "Time taken for batch:  4.588204383850098\n",
      "Data_size 0.08416748046875\n",
      "Time taken for batch:  9.326926708221436\n",
      "Data_size 0.03265380859375\n",
      "Time taken for batch:  1.6003568172454834\n",
      "Data_size 0.12432861328125\n",
      "Time taken for batch:  20.040581941604614\n",
      "Data_size 0.16009521484375\n",
      "Time taken for batch:  33.281569957733154\n",
      "Data_size 0.04705810546875\n",
      "Time taken for batch:  3.2347326278686523\n",
      "Data_size 0.06707763671875\n",
      "Time taken for batch:  5.977772235870361\n",
      "Data_size 0.10174560546875\n",
      "Time taken for batch:  13.549473524093628\n",
      "Data_size 0.04083251953125\n",
      "Time taken for batch:  2.421698808670044\n",
      "Data_size 0.06744384765625\n",
      "Time taken for batch:  6.034311771392822\n",
      "Data_size 0.15972900390625\n",
      "Time taken for batch:  32.98758316040039\n",
      "Data_size 0.05902099609375\n",
      "Time taken for batch:  4.880068778991699\n",
      "Data_size 0.08062744140625\n",
      "Time taken for batch:  8.579476833343506\n",
      "Data_size 0.05816650390625\n",
      "Time taken for batch:  4.603679180145264\n",
      "Data_size 0.15960693359375\n",
      "Time taken for batch:  32.91489768028259\n",
      "Data_size 0.04693603515625\n",
      "Time taken for batch:  3.2178616523742676\n",
      "Data_size 0.04608154296875\n",
      "Time taken for batch:  2.9190754890441895\n",
      "Data_size 0.11248779296875\n",
      "Time taken for batch:  16.45797085762024\n",
      "Data_size 0.12213134765625\n",
      "Time taken for batch:  19.49171781539917\n",
      "Data_size 0.07708740234375\n",
      "Time taken for batch:  7.9838783740997314\n",
      "Data_size 0.03594970703125\n",
      "Time taken for batch:  1.8850769996643066\n",
      "Data_size 0.14984130859375\n",
      "Time taken for batch:  29.00333571434021\n",
      "Data_size 0.04766845703125\n",
      "Time taken for batch:  3.2989182472229004\n",
      "Data_size 0.03668212890625\n",
      "Time taken for batch:  1.8958659172058105\n",
      "Data_size 0.08343505859375\n",
      "Time taken for batch:  9.134361743927002\n",
      "Data_size 0.05242919921875\n",
      "Time taken for batch:  3.790710210800171\n",
      "Data_size 0.05108642578125\n",
      "Time taken for batch:  3.546157121658325\n",
      "Data_size 0.14300537109375\n",
      "Time taken for batch:  26.460171699523926\n",
      "Data_size 0.09417724609375\n",
      "Time taken for batch:  11.781542301177979\n",
      "Data_size 0.05828857421875\n",
      "Time taken for batch:  4.6453094482421875\n",
      "Data_size 0.04827880859375\n",
      "Time taken for batch:  3.1950249671936035\n",
      "Data_size 0.06756591796875\n",
      "Time taken for batch:  6.0635693073272705\n",
      "Data_size 0.03314208984375\n",
      "Time taken for batch:  1.6126768589019775\n",
      "Data_size 0.11529541015625\n",
      "Time taken for batch:  17.264326333999634\n",
      "Data_size 0.10772705078125\n",
      "Time taken for batch:  15.244880437850952\n",
      "Data_size 0.04302978515625\n",
      "Time taken for batch:  2.6670608520507812\n",
      "Data_size 0.05548095703125\n",
      "Time taken for batch:  4.151543140411377\n",
      "Data_size 0.09588623046875\n",
      "Time taken for batch:  12.043299913406372\n",
      "Data_size 0.05023193359375\n",
      "Time taken for batch:  3.5243113040924072\n",
      "Data_size 0.04998779296875\n",
      "Time taken for batch:  3.401294469833374\n",
      "Data_size 0.10308837890625\n",
      "Time taken for batch:  13.874394178390503\n",
      "Data_size 0.09478759765625\n",
      "Time taken for batch:  11.86565637588501\n",
      "Data_size 0.10797119140625\n",
      "Time taken for batch:  15.2762291431427\n",
      "Data_size 0.11151123046875\n",
      "Time taken for batch:  16.291087865829468\n",
      "Data_size 0.16619873046875\n",
      "Time taken for batch:  35.8014919757843\n",
      "Data_size 0.10723876953125\n",
      "Time taken for batch:  15.20595121383667\n",
      "Data_size 0.08489990234375\n",
      "Time taken for batch:  9.777466535568237\n",
      "Data_size 0.05535888671875\n",
      "Time taken for batch:  4.199436902999878\n",
      "Data_size 0.04156494140625\n",
      "Time taken for batch:  2.410818099975586\n",
      "Data_size 0.16278076171875\n",
      "Time taken for batch:  34.28988289833069\n",
      "Data_size 0.05853271484375\n",
      "Time taken for batch:  4.80525279045105\n",
      "Data_size 0.10638427734375\n",
      "Time taken for batch:  14.759255409240723\n",
      "Data_size 0.10833740234375\n",
      "Time taken for batch:  15.40424919128418\n",
      "Data_size 0.09344482421875\n",
      "Time taken for batch:  11.54837441444397\n",
      "Data_size 0.09014892578125\n",
      "Time taken for batch:  10.74087381362915\n",
      "Data_size 0.05462646484375\n",
      "Time taken for batch:  4.105103492736816\n",
      "Data_size 0.09478759765625\n",
      "Time taken for batch:  11.767959833145142\n",
      "Data_size 0.06463623046875\n",
      "Time taken for batch:  5.6527745723724365\n",
      "Data_size 0.03839111328125\n",
      "Time taken for batch:  2.091156482696533\n",
      "Data_size 0.10113525390625\n",
      "Time taken for batch:  13.327857732772827\n",
      "Data_size 0.13385009765625\n",
      "Time taken for batch:  23.312475442886353\n",
      "Data_size 0.15838623046875\n",
      "Time taken for batch:  32.55910897254944\n",
      "Data_size 0.05206298828125\n",
      "Time taken for batch:  3.8814048767089844\n",
      "Data_size 0.14630126953125\n",
      "Time taken for batch:  27.701097011566162\n",
      "Data_size 0.05462646484375\n",
      "Time taken for batch:  4.22334098815918\n",
      "Data_size 0.07342529296875\n",
      "Time taken for batch:  7.142062664031982\n",
      "Data_size 0.07745361328125\n",
      "Time taken for batch:  7.961564064025879\n",
      "Data_size 0.07318115234375\n",
      "Time taken for batch:  7.139282941818237\n",
      "Data_size 0.08148193359375\n",
      "Time taken for batch:  8.781506061553955\n",
      "Data_size 0.09283447265625\n",
      "Time taken for batch:  11.348395109176636\n",
      "Data_size 0.05706787109375\n",
      "Time taken for batch:  4.463943243026733\n",
      "Data_size 0.06011962890625\n",
      "Time taken for batch:  4.861693382263184\n",
      "Data_size 0.04974365234375\n",
      "Time taken for batch:  3.38863468170166\n",
      "Data_size 0.05413818359375\n",
      "Time taken for batch:  3.953352451324463\n",
      "Data_size 0.06109619140625\n",
      "Time taken for batch:  5.003381252288818\n",
      "Data_size 0.03912353515625\n",
      "Time taken for batch:  2.156872510910034\n",
      "Data_size 0.05438232421875\n",
      "Time taken for batch:  3.9750030040740967\n",
      "Data_size 0.08734130859375\n",
      "Time taken for batch:  10.018075466156006\n",
      "Data_size 0.08233642578125\n",
      "Time taken for batch:  8.985105991363525\n",
      "Data_size 0.06256103515625\n",
      "Time taken for batch:  5.294112682342529\n",
      "Data_size 0.10235595703125\n",
      "Time taken for batch:  13.685425519943237\n",
      "Data_size 0.06793212890625\n",
      "Time taken for batch:  6.2285425662994385\n",
      "Data_size 0.09002685546875\n",
      "Time taken for batch:  10.660194158554077\n",
      "Data_size 0.09564208984375\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "model.to(device)\n",
    "# Loss function and optimizer\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    model.train() \n",
    "    total_loss = 0\n",
    "    len_data = 0\n",
    "    for batch in train_dataloader:\n",
    "        start_time = time.time()\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        inputs = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        targets = batch['labels'].to(device)\n",
    "        #print(\"Data_size\", get_tensor_size_in_mb(inputs)+ get_tensor_size_in_mb(attention_mask) + get_tensor_size_in_mb(targets))\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids = inputs, attention_mask=attention_mask,labels = targets) \n",
    "        loss = outputs.loss\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        end_time = time.time()\n",
    "        total_loss += loss\n",
    "        print(\"Time taken for batch: \", end_time - start_time)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_dataloader)}\")\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2d150c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
