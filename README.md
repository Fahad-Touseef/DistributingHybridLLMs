# DistributingHybridLLMs

## MambaFormer

SSM exhibit competitive performance their in-contxt learning(ICL) capabilities a remarkable emergent property of modern language model that enables task execution without paramter optimization, Transformers suffer in performance due to their time and memory complexities for long sequence. MambaFormer combines with attention blocks, surpassing individual models in tasks where they struggle independently.

## Jamba

Jambas is a hybrid decoder architecture that mixes Transformer and Mamba layers in addition to mixture-of-experts architecture.

## Manticore

Manticore Framework is designed for hybrid architecture by mixing components of pre-trained models. It uses projectors to align features across architecture and then applies a convex combination to align features from the hybrid models.
