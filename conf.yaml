model:
    n_embd: 512  
    n_layer: 13  # Updated to ensure the last layer is a Mamba layer
    n_dims: 64   
    interleave: False  # Changed from True to False
    mixed_attn: "mambaformer"  # "standard" or "mambaformer"
    n_positions: 512  # 1024
    ssm_cfg:  # Configuration for the state-space model (SSM) layer
        layer: "Mamba1"  # Changed to "Mamba1" for MambaFormer-style
        # block_size: 512
    attn_cfg:  # Configuration for the attention mechanism
        num_heads: 8
        # dropout: 0.1
    attn_layer_idx: [1, 3, 5, 7, 9, 11]  # MHA layers are at odd indices; Mamba layers are at even indices
    # fused_add_norm: True  # Enable fused add + norm for performance
    # residual_in_fp32: True  # Use FP32 for residual connections

dataloader:
    tokenizer_name: "gpt2"
    dataset_name: "wikitext"
    dataset_config: "wikitext-103-v1"
    seq_len: 512  # 1024
    batch_size: 4

training:
    learning_rate: 0.0001
    # save_every_steps: 10000     (Not being used right now i.e. rn only final model is saved)
    train_steps: 500001
    epochs: 10  
    out_dir: "./models"

wandb:
    project: "mambaformer_training"
    entity: "fahad15-university-of-wisconsin-madison"
    notes: "Distributing training of mambaformer"
    log_every_steps: 10000